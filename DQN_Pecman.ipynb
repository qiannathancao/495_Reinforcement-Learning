{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MsPacman-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape #{lenth,width,channel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space #{actions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspacman_color = np.array([210,163,74]).mean()\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2,::2] \n",
    "    # 1:176:2 means from 1 thru 172 with step size: 2\n",
    "    # ::2 means: do above process for the 1st and 2nd dimention,3rd dimension no change\n",
    "    # img.shape: (88,80,3)\n",
    "    img = img.mean(axis=2) # gray convertion\n",
    "    img[img==mspacman_color]=0 #Improve contrast\n",
    "    img = (img - 128)/128 -1 # regulize to -1 and 1\n",
    "    return img.reshape(88,80,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN structure: \n",
    "# layer1: 32,8x8+4stride\n",
    "# layer2: 64,4x4+2stride\n",
    "# layer3: 64,3x3+1stride\n",
    "# fully conn: 512 units\n",
    "# fully conn: 9 nnits\n",
    "\n",
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "conv_n_maps = [32,64,64]\n",
    "conv_kernel_size = [(8,8),(4,4),(3,3)]\n",
    "conv_strides = [4,2,1]\n",
    "conv_paddings = ['SAME']*3 \n",
    "# ['SAME', 'SAME', 'SAME']\n",
    "conv_activation = [tf.nn.relu]*3 \n",
    "# [<function tensorflow.python.ops.gen_nn_ops.relu>,\n",
    "#  <function tensorflow.python.ops.gen_nn_ops.relu>,\n",
    "#  <function tensorflow.python.ops.gen_nn_ops.relu>]\n",
    "n_hidden_in = 64*11*10 # 64 (11x10)s in layers\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n # 9 actions {up, down,left......}\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import convolution2d,fully_connected\n",
    "\n",
    "# We will have 2 DQNs, one as actor, and another as critic\n",
    "# Build q_network() to create DQNs\n",
    "def q_network(X_state,scope):\n",
    "    prev_layer = X_state\n",
    "    conv_layers = []\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        for n_maps, kernel_size, stride,padding,activation in zip(conv_n_maps,\n",
    "                                                                  conv_kernel_size,\n",
    "                                                                  conv_strides,\n",
    "                                                                  conv_paddings,\n",
    "                                                                  conv_activation\n",
    "                                                                 ):\n",
    "            prev_layer = convolution2d(prev_layer,\n",
    "                                       num_outputs=n_maps,\n",
    "                                       kernel_size = kernel_size,\n",
    "                                       stride = stride,\n",
    "                                       padding = padding,\n",
    "                                       activation_fn = activation,\n",
    "                                       weights_initializer = initializer\n",
    "                                      )\n",
    "            conv_layers.append(prev_layer)\n",
    "        \n",
    "        last_conv_layer_flat = tf.reshape(prev_layer,\n",
    "                                          shape=[-1,n_hidden_in])\n",
    "        hidden = fully_connected(last_conv_layer_flat,\n",
    "                                 n_hidden,\n",
    "                                 activation_fn = hidden_activation,\n",
    "                                 weights_initializer = initializer\n",
    "                                )\n",
    "        outputs = fully_connected(hidden,\n",
    "                                  n_outputs,\n",
    "                                  activation_fn = None,\n",
    "                                  weights_initializer = initializer\n",
    "                                 )\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                       scope = scope.name\n",
    "                                      )\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]:var\n",
    "                              for var in trainable_vars\n",
    "                             }\n",
    "    return outputs,trainable_vars_by_name\n",
    "           #: output is q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN cost function_critics : J(theta_critic) \n",
    "#### J(theta_critic) = 1/m * Sum [(y - q_value(s,a,theta))**2]\n",
    "### NN trained value: y\n",
    "#### y = reward + discount_rate * max_q_value(next_state, next_action, theta_actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder creation, copy critic's DQN to actor's DQN\n",
    "\n",
    "X_state = tf.placeholder(tf.float32, \n",
    "                         shape=[None,input_height,input_width,input_channels])\n",
    "actor_q_values, actor_vars = q_network(X_state,scope=\"q_networks/actor\")\n",
    "critic_q_values , critic_vars = q_network(X_state,scope=\"q_network/critic\")\n",
    "\n",
    "# copy critic to actor\n",
    "copy_ops = [actor_var.assign(critic_vars[var_name]) for var_name, actor_var in actor_vars.items()]\n",
    "copy_critic_to_actor = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder critic's DQN training\n",
    "# one_hot for recording action x critic's q_value\n",
    "X_action = tf.placeholder(tf.int32,shape=[None])\n",
    "q_value = tf.reduce_sum(critic_q_values*tf.one_hot(X_action,n_outputs),axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding training process:\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Q-value will be added to placeholder\n",
    "y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "# cost function is MSE \n",
    "cost = tf.reduce_mean(tf.square(y-q_value))\n",
    "\n",
    "global_step = tf.Variable(0,trainable=False,name='global_step')\n",
    "\n",
    "# AdamOptimize to do optimization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(cost,global_step = global_step)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need tool: deque 弹出数据，当replay 快满的时候\n",
    "# Need function: randomly sampling from replay\n",
    "\n",
    "from collections import deque\n",
    "replay_memory_size = 10000\n",
    "replay_memory = deque([],maxlen = replay_memory_size)\n",
    "\n",
    "def sample_memories(batch_size):\n",
    "    indices = rnd.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[],[],[],[]] # state, action,reward,next_state,continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols,memory):\n",
    "            col.apppend(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0],cols[1],cols[2].reshape(-1,1),cols[3],cols[4].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor to explore game using epsilon-greedy\n",
    "# in 50000 training steps, reduce epsilon from 1 to 0.05 \n",
    "\n",
    "eps_min = 0.05\n",
    "eps_max = 1\n",
    "eps_decay_steps = 5000\n",
    "\n",
    "def epsilon_greedy(q_value,step):\n",
    "    epsilon = max(eps_min,eps_max - (eps_max-eps_min)*step/eps_decay_steps)\n",
    "    if rnd.rand()<epsilon:\n",
    "        return rnd.randint(n_outputs) # 随机的动作\n",
    "    else: \n",
    "        return np.argmax(q_value) # 最优的动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING START... LET'S GOOOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 10000 # 总共训练步长（数）\n",
    "training_start = 1000 # GAME 开始1000次迭代开始训练\n",
    "training_interval = 3 # 然后 每隔3次迭代，训练一次\n",
    "save_steps = 50 # 每训练50步，保存一下模型\n",
    "copy_steps = 25 # 每训练25步，复制 critic's Q_value 给 actor\n",
    "discount_rate = 0.95\n",
    "skip_start = 90 # 跳过游戏的开始时间\n",
    "batch_size = 50\n",
    "iteration = 0\n",
    "checkpoint_path = (\"./dqn_pacman.ckpt\")\n",
    "done = True # 当true，env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow.session 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        saver.restore(sess,checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration +=1\n",
    "        if done: # 若 GAMEOVER，restart new game\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start): # skip game foreplay\n",
    "                obs,reward,done,info = env.step(0)\n",
    "                state = preprocess_observation(obs)\n",
    "                \n",
    "# actor 要做的事情：\n",
    "    q_values = actor_q_values.eval(eed_dict = {X_state: [state]})\n",
    "    action = epsilon_greedy(q_values,step)\n",
    "\n",
    "# actor 开始玩游戏:\n",
    "    obs,reward, done, info = env.step(action) # action 后 \n",
    "    next_state = preprocess_observation(obs)  # state 后\n",
    "    print (reward)\n",
    "    print (q_values)\n",
    "    \n",
    "# 记录下 发生的事 in Replay Memory\n",
    "    replay_memory.append((state,action,next_state,1-done))\n",
    "    state = next_state\n",
    "    \n",
    "    #if (iteration < training_start) or (iteration%training_interval != 0): \n",
    "        #continue\n",
    "\n",
    "# critic to learn:\n",
    "    X_state_val,X_action_val,rewards,X_next_state_val,continues = (sample_memories(batch_size))\n",
    "    next_q_values = actor_q_values.eval(feed_dict = {X_state:X_next_state_val})\n",
    "    max_next_q_values = np.max(next_q_values,axis=1,keepdims=True)\n",
    "    y_val = rewards + continues*discount_rate*max_next_q_values\n",
    "    training_op.run(feed_dict = {X_state: X_state_val,\n",
    "                                 X_action: X_action_val,\n",
    "                                 y: y_val})\n",
    "    \n",
    "# Copy critic's q_value to actor:\n",
    "    if step % copy_steps == 0:\n",
    "        copy_critic_to_actor.run()\n",
    "        \n",
    "# Save Model\n",
    "    if step % save_steps == 0:\n",
    "        saver.save(sess,checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
