{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "env.reset()\n",
    "\n",
    "#Hyperparameters\n",
    "H_SIZE = 10 #Number of hidden layer neurons\n",
    "batch_size = 5 #Update Params after every 5 episodes\n",
    "learning_r = 0.01 #Learning Rate\n",
    "GAMMA = 0.95 #Discount factor\n",
    "\n",
    "INPUT_DIM = 4 #Input dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Network to define moving left or right\n",
    "input = tf.placeholder(tf.float32, [None,INPUT_DIM] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[INPUT_DIM, H_SIZE],\n",
    "           initializer=tf.contrib.layers.xavier_initializer()) # weight for hidden layer\n",
    "layer1 = tf.nn.relu(tf.matmul(input,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H_SIZE, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer()) # weight for output layer\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "# Define network needed for learning a good policy.\n",
    "tvars = tf.trainable_variables() # for storing weights caches\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\") # advantage is total reward over time\n",
    "\n",
    "# The loss function sends the weights in the direction of making actions \n",
    "# that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability)) # Entropy generation\n",
    "\n",
    "loss = -tf.reduce_mean(loglik * advantages) # advantage is total reward over time:\n",
    "newGrads = tf.gradients(loss,tvars) # get dloss_dtvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization \n",
    "\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_r) # Adam optimizer\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\") # Placeholders for final gradients once update happens\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad] # Update Params after every mini batch: 5 episodes\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)): # calculate backward, last step's reward enjoys least impact from discount\n",
    "        running_add = running_add * GAMMA + r[t] # aggregating reward iteratively \n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# xs:input_x, \n",
    "# drs: store reward after action is taken,\n",
    "# ys: y = 1 if action == 0 else 0\n",
    "xs,drs,ys = [],[],[] #Arrays to store parameters till an update happens\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 100\n",
    "initial = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 14.800000.  Total average reward 14.800000.\n",
      "Average reward for episode 14.600000.  Total average reward 14.206000.\n",
      "Average reward for episode 26.600000.  Total average reward 13.761700.\n",
      "Average reward for episode 27.800000.  Total average reward 13.351615.\n",
      "Average reward for episode 19.200000.  Total average reward 12.876034.\n",
      "Average reward for episode 28.800000.  Total average reward 12.520233.\n",
      "Average reward for episode 16.000000.  Total average reward 12.054221.\n",
      "Average reward for episode 16.400000.  Total average reward 11.615510.\n",
      "Average reward for episode 18.200000.  Total average reward 11.216734.\n",
      "Average reward for episode 24.200000.  Total average reward 10.897898.\n",
      "Average reward for episode 18.000000.  Total average reward 10.533003.\n",
      "Average reward for episode 22.200000.  Total average reward 10.228353.\n",
      "Average reward for episode 23.200000.  Total average reward 9.948935.\n",
      "Average reward for episode 23.800000.  Total average reward 9.689488.\n",
      "Average reward for episode 25.000000.  Total average reward 9.455014.\n",
      "Average reward for episode 33.400000.  Total average reward 9.316263.\n",
      "Average reward for episode 22.800000.  Total average reward 9.078450.\n",
      "Average reward for episode 16.800000.  Total average reward 8.792527.\n",
      "Average reward for episode 22.200000.  Total average reward 8.574901.\n",
      "Average reward for episode 22.800000.  Total average reward 8.374156.\n",
      "101 Episodes completed.\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(initial)\n",
    "    input_initial = env.reset() # Initial state of the environment\n",
    "\n",
    "    # Array to store gradients for each min-batch step\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        if reward_sum/batch_size > 100 or rendering == True :     #Render environment only after avg reward reaches 100\n",
    "            #env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        # Format the state for placeholder\n",
    "        x = np.reshape(input_initial,[1,INPUT_DIM]) # introduce initial state to shape [1,4]\n",
    "        \n",
    "        # Run policy network \n",
    "        tfprob = sess.run(probability,feed_dict={input: x}) #  calculate sigmoid for initial state\n",
    "        action = 1 if np.random.uniform() < tfprob else 0 # make an initial move \n",
    "        \n",
    "        xs.append(x) # store the present state to the state list\n",
    "        y = 1 if action == 0 else 0\n",
    "        ys.append(y) # store \n",
    "\n",
    "        # take action for the state\n",
    "        input_initial, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward) # store reward after action is taken\n",
    "\n",
    "        if done: # done means collaps, then entering to a brand new episode\n",
    "            episode_number += 1\n",
    "            # Stack the memory arrays to feed in session\n",
    "            epx = np.vstack(xs) # stack all recorded state in one list\n",
    "            epy = np.vstack(ys) # ??\n",
    "            epr = np.vstack(drs)# stack all recorded reward in one list\n",
    "            \n",
    "            xs,drs,ys = [],[],[] #Reset Arrays\n",
    "\n",
    "            # Compute the discounted reward\n",
    "            discounted_epr = discount_rewards(epr) # expected reward is the aggregation of reward with discounted factor\n",
    "            \n",
    "            # normalize and standardize the reward\n",
    "            discounted_epr -= np.mean(discounted_epr) # scalized the dicounted reward\n",
    "            discounted_epr /= np.std(discounted_epr) # standardize the dicount reward\n",
    "            \n",
    "            # Get and save the gradient\n",
    "            tGrad = sess.run(newGrads,feed_dict={input: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # Update Params after Min-Batch number of episodes\n",
    "            if episode_number % batch_size == 0: \n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                # Print details of the present model\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.95 + reward_sum * 0.01\n",
    "                print ('Average reward for episode %f.  Total average reward %f.' % (reward_sum/batch_size, running_reward/batch_size))\n",
    "                \n",
    "                if reward_sum/batch_size > 200: \n",
    "                    print (\"Task solved in\",episode_number,'episodes')\n",
    "                    break\n",
    "                    \n",
    "                reward_sum = 0\n",
    "            \n",
    "            input_initial = env.reset()\n",
    "        \n",
    "print (episode_number,'Episodes completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
