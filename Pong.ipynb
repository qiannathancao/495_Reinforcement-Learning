{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_obs = 80 * 80           # dimensionality of observations\n",
    "h = 200                   # number of hidden layer neurons\n",
    "n_actions = 3             # number of available actions\n",
    "learning_rate = 1e-3\n",
    "gamma = .95               # discount factor for reward\n",
    "decay = 0.99              # decay rate for RMSProp gradients\n",
    "save_path='models/pong.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamespace \n",
    "env = gym.make(\"Pong-v0\") # environment info\n",
    "observation = env.reset()\n",
    "prev_x = None\n",
    "xs,rs,ys = [],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0  # erase background (background type 1)\n",
    "    I[I == 109] = 0  # erase background (background type 2)\n",
    "    I[I != 0] = 1    # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "tf_model = {}\n",
    "with tf.variable_scope('layer_one',reuse=False):\n",
    "    xavier_l1 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(n_obs), dtype=tf.float32)\n",
    "    tf_model['W1'] = tf.get_variable(\"W1\", [n_obs, h], initializer=xavier_l1)\n",
    "with tf.variable_scope('layer_two',reuse=False):\n",
    "    xavier_l2 = tf.truncated_normal_initializer(mean=0, stddev=1./np.sqrt(h), dtype=tf.float32)\n",
    "    tf_model['W2'] = tf.get_variable(\"W2\", [h,n_actions], initializer=xavier_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 0. RL's reward function (for interation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf operations: Rewards, \n",
    "def tf_discount_rewards(tf_r): #tf_r ~ [game_steps,1]\n",
    "    discount_f = lambda a, v: a*gamma + v;\n",
    "    tf_r_reverse = tf.scan(discount_f, tf.reverse(tf_r,[True, False]))\n",
    "    tf_discounted_r = tf.reverse(tf_r_reverse,[True, False])\n",
    "    return tf_discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### My code for tf_discount_rewards(tf_r):\n",
    "\n",
    "#def tf_discount_rewards(r):\n",
    "#    discounted_r = np.zeros_like(r)\n",
    "#    running_add = 0\n",
    "#    for t in reversed(range(0, len(r))):\n",
    "#    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "#    running_add = running_add * gamma + r[t]\n",
    "#    discounted_r[t] = running_add\n",
    "#    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Placeholder:  input, label, and reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf placeholders\n",
    "tf_x = tf.placeholder(dtype=tf.float32, shape=[None, n_obs],name=\"tf_x\")\n",
    "tf_y = tf.placeholder(dtype=tf.float32, shape=[None, n_actions],name=\"tf_y\")\n",
    "tf_epr = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"tf_epr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf processing: Rewards (need tf_discounted_epr for policy gradient)\n",
    "tf_discounted_epr = tf_discount_rewards(tf_epr)\n",
    "tf_mean, tf_variance= tf.nn.moments(tf_discounted_epr, [0], shift=None, name=\"reward_moments\")\n",
    "# tf_mean is function \n",
    "\n",
    "tf_discounted_epr -= tf_mean\n",
    "tf_discounted_epr /= tf.sqrt(tf_variance + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. NN modeling & Optimizating gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf operation: policy\n",
    "def tf_policy_forward(x): #x ~ [1,D]\n",
    "    h = tf.matmul(x, tf_model['W1'])\n",
    "    h = tf.nn.relu(h)\n",
    "    logp = tf.matmul(h, tf_model['W2'])\n",
    "    p = tf.nn.softmax(logp)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf optimizer op\n",
    "tf_aprob = tf_policy_forward(tf_x) # calculate output, which is a prob\n",
    "loss = tf.nn.l2_loss(tf_y-tf_aprob)# cost function to compare calculated output vs label\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay) # choose the optimizer to implement\n",
    "\n",
    "tf_grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=tf_discounted_epr)\n",
    "# compute grads based on: model_loss, stored_weights, policy_gradients_loss \n",
    "\n",
    "train_op = optimizer.apply_gradients(tf_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution phase\n",
    "###### Initialization & Create training model save_load path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-69e454774ad5>:6: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "no saved model to load. starting new session\n"
     ]
    }
   ],
   "source": [
    "# tf graph initialization\n",
    "sess = tf.InteractiveSession()\n",
    "#tf.initialize_all_variables().run()\n",
    "tf.global_variables_initializer().run()\n",
    "# try load saved model\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "load_was_success = True # yes, I'm being optimistic\n",
    "try:\n",
    "    save_dir = '/'.join(save_path.split('/')[:-1])\n",
    "    ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "    load_path = ckpt.model_checkpoint_path\n",
    "    saver.restore(sess, load_path)\n",
    "except:\n",
    "    print (\"no saved model to load. starting new session\")\n",
    "    load_was_success = False\n",
    "else:\n",
    "    print (\"loaded model: {}\".format(load_path))\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    episode_number = int(load_path.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0: reward: -21.0, mean reward: -21.000000\n",
      "\tep 1: reward: -21.0\n",
      "\tep 2: reward: -21.0\n",
      "\tep 3: reward: -21.0\n",
      "\tep 4: reward: -19.0\n",
      "\tep 5: reward: -21.0\n",
      "\tep 6: reward: -21.0\n",
      "\tep 7: reward: -21.0\n",
      "\tep 8: reward: -21.0\n",
      "\tep 9: reward: -21.0\n",
      "ep 10: reward: -20.0, mean reward: -14.234079\n",
      "\tep 11: reward: -21.0\n",
      "\tep 12: reward: -20.0\n",
      "\tep 13: reward: -21.0\n",
      "\tep 14: reward: -21.0\n",
      "\tep 15: reward: -21.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d138f6132d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# preprocess the observation, set input to network to be difference image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleImageViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    343\u001b[0m         gl.glTexParameteri(gl.GL_TEXTURE_2D, \n\u001b[1;32m    344\u001b[0m             gl.GL_TEXTURE_MAG_FILTER, gl.GL_NEAREST)\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mtexture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_texture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mtexture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mtexture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mget_texture\u001b[0;34m(self, rectangle, force_rectangle)\u001b[0m\n\u001b[1;32m    857\u001b[0m             self._current_texture = self.create_texture(Texture,\n\u001b[1;32m    858\u001b[0m                                                         \u001b[0mrectangle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                                                         force_rectangle)\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_texture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mcreate_texture\u001b[0;34m(self, cls, rectangle, force_rectangle)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0minternalformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_internalformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         texture = cls.create(self.width, self.height, internalformat,\n\u001b[0;32m--> 844\u001b[0;31m                              rectangle, force_rectangle)\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mtexture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, width, height, internalformat, rectangle, force_rectangle, min_filter, mag_filter)\u001b[0m\n\u001b[1;32m   1547\u001b[0m             \u001b[0mtexture_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m             \u001b[0mtexture_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nearest_pow2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1550\u001b[0m             \u001b[0mtexture_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nearest_pow2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pyglet/image/__init__.py\u001b[0m in \u001b[0;36m_nearest_pow2\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m   1425\u001b[0m     \u001b[0;31m# From http://graphics.stanford.edu/~seander/bithacks.html#RoundUpPowerOf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;31m# Credit: Sean Anderson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "while True:\n",
    "    if True: env.render()\n",
    "\n",
    "    # preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(n_obs)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # stochastically sample a policy from the network\n",
    "    feed = {tf_x: np.reshape(x, (1,-1))}\n",
    "    aprob = sess.run(tf_aprob,feed) ; aprob = aprob[0,:]\n",
    "    action = np.random.choice(n_actions, p=aprob)\n",
    "    label = np.zeros_like(aprob) ; label[action] = 1\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action+1)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    # record game history\n",
    "    xs.append(x) ; ys.append(label) ; rs.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        # update running reward\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.95 + reward_sum * 0.01\n",
    "        \n",
    "        # parameter update\n",
    "        feed = {tf_x: np.vstack(xs), tf_epr: np.vstack(rs), tf_y: np.vstack(ys)}\n",
    "        _ = sess.run(train_op,feed)\n",
    "        \n",
    "        # print progress console\n",
    "        if episode_number % 10 == 0:\n",
    "            print ('ep {}: reward: {}, mean reward: {:3f}'.format(episode_number, reward_sum, running_reward))\n",
    "        else:\n",
    "            print ('\\tep {}: reward: {}'.format(episode_number, reward_sum))\n",
    "        \n",
    "        # bookkeeping\n",
    "        xs,rs,ys = [],[],[] # reset game history\n",
    "        episode_number += 1 # the Next Episode\n",
    "        observation = env.reset() # reset env\n",
    "        reward_sum = 0\n",
    "        if episode_number % 50 == 0:\n",
    "            saver.save(sess, save_path, global_step=episode_number)\n",
    "            print (\"SAVED MODEL #{}\".format(episode_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
